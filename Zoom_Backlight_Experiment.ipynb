{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom Backlighting Experiment Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "<ul>\n",
    "    <li> Should do some sort of spatiotemporal processing maybe?? (See motion 2 compact slides)\n",
    "        - Might find weird dropped frames, like zoom, up to me to decide if that is an issue or not. this might be a way to make it more stable, Can also use to improve detection. , but notice it is dropping 1 in 5 detections. so could say if detection is different enough, then reject the detection. \n",
    "        - could have temporal rules, if the pure photos doesn't work as well by itself.\n",
    "    <li> Trouble with measuring backlit ness (see metrics in new resources)\n",
    "    <li> When to do the processing? Only when we detect that our image is backlit? \n",
    "        - could try to come up with a test for this, could run haar cascade, if detect face then look look at average brightness in face bounding box, and see what average brightness elsewhere is. \n",
    "        - or if haar find a face, then know it is okay. If not, then find my stuff. \n",
    "        - Or could assume that you have a thing where it is a user option. And have the user figure it out \n",
    "    <li> Haar gives a square, do we want whole outline of person?\n",
    "    <li> Haar doesnâ€™t work with rotated heads (should prob just find another method for this)\n",
    "</ul>\n",
    "\n",
    "Issues:\n",
    "- How can we measure how good it is compared to Zoom if we don't have a good way to route normal photos through the Zoom algorithm? Maybe \"train\" the algo on other dataset, and then use my own data for the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes from meeting November 16\n",
    "- Haar cascade alternative??\n",
    "    - Try lighteing up whole image (do straight up brightness operator), might have background be oversaturated. \n",
    "    - might also need to stack filters. eg. gain and then sharpness, or squaring of pixel values, (more separation), but the more you do stuff, the more you potentailly create weird artifacts. \n",
    "    - spend some time playing around with this. \n",
    "    - if haar is over grayscale, amkes it easier to implement, when you saturate over specfici color channels, you start to get wried things, need to make sure you don't overspill, python might have issues if over 255.\n",
    "    - max or min function to cut off values if this is the case. \n",
    "    - a lot of the early filters we talked about could be useful. \n",
    "    - Set of filters taht may be helpful, Prof. wloka will send. \n",
    "    - Goal for this is to map face area to color range that haar can recognize. \n",
    "    - other option is to train haar cascade over low light images. \n",
    "- Could do this or could focus on getting face detection up and running. \n",
    "- If this ends up working to make the face look good, could also keep this over the segmentation that I find and then put in the old background. \n",
    "- High dynamic range photography TODO look this up. \n",
    "    - take photos at a bunch of differen tlighting, then combine together into one image. \n",
    "    - HDR algorithm. \n",
    "- Might not be an issue that we just have the bounding box. We could just segment the person. \n",
    "    - Mask RCNN for a person? Maybe ???\n",
    "- IF we want to go with a learned method, that's okay as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used\n",
    "- [The Haar Cascade tutorial](https://learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/). Also includes some info about non-haar cascade face detection models.\n",
    "\n",
    "### Might Use\n",
    "- Yale Dataset with differing levels of backlit people's faces: [website](http://vision.ucsd.edu/~iskwak/ExtYaleDatabase/ExtYaleB.html). Has illumination conditions labeled, etc.\n",
    "- Some resources for doing low light image enhancement: [Github](https://github.com/dawnlh/awesome-low-light-image-enhancement)\n",
    "    - Other ways to do facial detection\n",
    "        - [Recurrent Exposure Generation for Low-Light Face Detection\n",
    "](https://arxiv.org/abs/2007.10963)\n",
    "        - [HLA Face]( https://daooshee.github.io/HLA-Face-Website/)\n",
    "        - [Single-Stage_Face_Detection_Under_Extremely_Low-Light_Conditions](https://openaccess.thecvf.com/content/ICCV2021W/RLQ/papers/Yu_Single-Stage_Face_Detection_Under_Extremely_Low-Light_Conditions_ICCVW_2021_paper.pdf)\n",
    "    - Benchmark? [WIDER FACE benchmark](http://shuoyang1213.me/WIDERFACE/)\n",
    "- Google stuff about doing directional light enhancement on photos. [Blog post](https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html) and [facial detection keypoint model documentation](https://google.github.io/mediapipe/solutions/face_mesh.html)\n",
    "</ul>\n",
    "- Some discussion about how to do automatic lighting correction in open cv [Stack Overflow](https://stackoverflow.com/questions/56905592/automatic-contrast-and-brightness-adjustment-of-a-color-photo-of-a-sheet-of-pape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make necessary imports. Make sure we run the jupyter notebook instance using the (base) environment, else some of these packages won't be installed yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pylab\n",
    "from glob import glob\n",
    "import cv2\n",
    "from icecream import ic\n",
    "import os\n",
    "import unittest\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "pylab.rcParams['figure.figsize'] = (12, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our images! We load them into a dictionary for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file 'images/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-be04f88fdf54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimgDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgFolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimgDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgFolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# print out the keys for future reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2965\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccept_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2966\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2967\u001b[0;31m     raise UnidentifiedImageError(\n\u001b[0m\u001b[1;32m   2968\u001b[0m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2969\u001b[0m     )\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file 'images/.DS_Store'"
     ]
    }
   ],
   "source": [
    "imgFolder = \"images\"\n",
    "imgDict = {}\n",
    "for filename in os.listdir(imgFolder):\n",
    "    imgDict[filename] = np.array(Image.open(imgFolder + '/' + filename))\n",
    "[key for key in imgDict.keys()] # print out the keys for future reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the first image we are looking at, which is just a typical backlit image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "plt.imshow(imgDict['badBrightnessPhoto.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the other four images, all of which have differing levels of backlit adjustment from Zoom's own algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(imgDict[\"noAdjustLowLight.png\"])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(imgDict['autoAdjustForLowLight.png'])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(imgDict['manualAdjustLowLightMax.png'])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(imgDict['adjustForLowLightManualHalfway.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face detection (Haar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FaceDetect holds methods to detect the face from a picture or video stream. I wrote this class by adapting it from some previous work I did with Haar cascades (scroll to the bottom of [this file](https://github.com/tennisoctocat/push-up-poker/blob/main/training/FacialFeatures.ipynb), which I had previously adapted from this [tutorial](https://learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FaceDetect holds methods to detect the face from a picture or video stream.\"\"\"\n",
    "\n",
    "class FaceDetect():\n",
    "\tdef __init__(self):\n",
    "\t\tself.faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\t\tself.faces = [] # faces in form of x, y, width, height\n",
    "\t\tself.shapeOfLastDetectedImg = (0, 0)\n",
    "\n",
    "\t# Public method, lets you use whatever implmentation you want. \n",
    "\tdef getNewFrame(self, img, timeStep=0):\n",
    "\t\t\"\"\"Gets the new frame to show on the screen. Called by external classes, etc.\"\"\"\n",
    "\t\t# Always run detection of img's shape changed. Otherwise we might get\n",
    "\t\t# an index out of bounds exception for when the old faces run off the edges \n",
    "\t\t\t# of the new frame.\n",
    "        # Timestep included for video processing (only do processing every 20 timesteps)\n",
    "\t\tif timeStep % 20 == 0 or np.array(img).shape != self.shapeOfLastDetectedImg:\n",
    "\t\t\tself._get_faces_with_haar(img)\n",
    "\t\t\treturn self._draw_faces(img)\n",
    "\t\treturn self._draw_faces(img)\n",
    "\n",
    "\tdef _draw_faces(self, img):\n",
    "\t\t\"\"\"Draws a white box for every face in the self.faces array\"\"\"\n",
    "        \n",
    "\t\tif self.faces is None:\n",
    "\t\t\treturn img\n",
    "        \n",
    "\t\tnumChannels = img.shape[-1]\n",
    "\t\tfor face in self.faces:\n",
    "\t\t\tx,y,w,h = face\n",
    "\t\t\timg[y: y + h, x] = [255] * numChannels # Left off here, need to draw the actual bounding boxes. \n",
    "\t\t\timg[y: y + h, x + w] = [255] * numChannels# Left off here, need to draw the actual bounding boxes. \n",
    "\t\t\timg[y, x:x + w] = [255] * numChannels\n",
    "\t\t\timg[y + h, x:x + w] = [255] * numChannels\n",
    "\t\treturn img\n",
    "\n",
    "\tdef _get_faces_with_haar(self, img):\n",
    "\t\t\"\"\"Uses haar cascades to detect faces and save them in the self.faces array\"\"\"\n",
    "\t\tif img is None:\n",
    "\t\t\treturn \n",
    "\n",
    "\t\timg = np.array(img)\n",
    "\n",
    "\t\t# Detect faces\n",
    "\t\tself.faces = self.faceCascade.detectMultiScale(img, minNeighbors=3, minSize=(int(img.shape[0]/10), int(img.shape[0]/10)))\n",
    "\t\tself.shapeOfLastDetectedImg = img.shape\n",
    "\n",
    "\t\t# Print so we know what is happening\n",
    "\t\tif len(self.faces) > 1:\n",
    "\t\t\tprint(\"faces greater than 1 \")\n",
    "\t\telif len(self.faces) == 0:\n",
    "\t\t\tprint(\"no face found \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `FaceDetect` instance in order to detect and display images with bounding boxes for the faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceDetect = FaceDetect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that Haar cascades doesn't do so well when I'm wearing a mask and it is dark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badBrightnessFacesImg = faceDetect.getNewFrame(imgDict['badBrightnessPhoto.png'])\n",
    "plt.imshow(badBrightnessFacesImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, it works just fine on normal photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalFacesImg = faceDetect.getNewFrame(imgDict['well-lit1.jpg'])\n",
    "plt.imshow(normalFacesImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to mess with photo before using Haar cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faceDetect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6023c2941bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbadBrightnessFacesImg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaceDetect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNewFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'badBrightnessPhoto.png'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbadBrightnessFacesImg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'faceDetect' is not defined"
     ]
    }
   ],
   "source": [
    "badBrightnessFacesImg = faceDetect.getNewFrame(imgDict['badBrightnessPhoto.png'])\n",
    "plt.imshow(badBrightnessFacesImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
